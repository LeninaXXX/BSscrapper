{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> || articles : 55 / sections : 13 / headers : 1 / footers : 1 / asides : 0 / h1s : 1 / h2s : 66 / h3s : 35 / h4s : 0 / h5s : 0 / h6s : 0\n"
     ]
    }
   ],
   "source": [
    "import requests, bs4 as bs, lxml, sys\n",
    "sys.path.append('..'); import utils.BSutils\n",
    "\n",
    "httpaddr = \"http://la100.cienradios.com/\"\n",
    "FILEDUMP = 'dump/La100_dump.txt'\n",
    "\n",
    "requests_ret = requests.get(httpaddr)\n",
    "\n",
    "print(requests_ret, end = ' || ')\n",
    "soup = bs.BeautifulSoup(requests_ret.text, 'lxml')\n",
    "for discard_tag in (\"script\", \"style\"): # discard <script> and <style> tags\n",
    "    for t in soup.find_all(discard_tag): t.extract()\n",
    "tags_tuple = ('article', 'section', 'header', 'footer', 'aside') + tuple(('h' + str(i) for i in range(1, 7)))\n",
    "tags = {tag_str : soup.find_all(tag_str) for tag_str in tags_tuple}\n",
    "for i, tag in enumerate(tags):\n",
    "    print(f\"{tag}s : {len(tags[tag])}\", sep = '', end = '\\n' if i == len(tags) - 1 else ' / ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# These routines outputs a report on a given tag plus all their descendants accompanied with their respective attributes\n",
    "def fmt_attrs(attrs_dict, width = 80, excl_attrs = []):\n",
    "    key_width = max((len(k) for k in attrs_dict)) if attrs_dict else 0\n",
    "    head_width = len(\"{'\") + key_width + len(\"' : '\")\n",
    "    chunk_len = width - head_width\n",
    "    strs_list = []\n",
    "\n",
    "    for i, key in enumerate(attrs_dict):\n",
    "        if key in excl_attrs:\n",
    "            continue\n",
    "        head_str = (\"{'\" if i == 0 else \" '\") + (\"%-\" + str(key_width) + \"s\") % (key, ) + \"' : '\"\n",
    "        val_chunks = tuple((str(attrs_dict[key])[pos : pos + chunk_len] for pos in range(0, len(str(attrs_dict[key])), chunk_len)))\n",
    "        \n",
    "        for (j, chunk) in enumerate(val_chunks):\n",
    "            strs_list.append(\n",
    "                head_str + chunk if j == 0 else\n",
    "                ' ' * head_width + chunk\n",
    "            )\n",
    "        if strs_list:\n",
    "            strs_list.append(strs_list.pop() + \"'\")    \n",
    "\n",
    "    if strs_list: \n",
    "        strs_list.append(strs_list.pop() + \"'\")\n",
    "\n",
    "    return strs_list\n",
    "\n",
    "def fmt_str(text, width):\n",
    "    if hasattr(text, '__str__'):\n",
    "        return (text[i : i + width] for i in range(0, len(text), width))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def tree_dump(tag, pfx, gettext_tags = [], excl_tags = [], excl_attrs = [], file = sys.stdout):\n",
    "    stack = []\n",
    "    stack.extend(reversed([ch for ch in tag.children if isinstance(ch, bs.element.Tag)]))\n",
    "\n",
    "    attrs_report = fmt_attrs(tag.attrs, 135, excl_attrs) if fmt_attrs(tag.attrs, 135, excl_attrs) != [] else ['{}']\n",
    "    for i, chunk in enumerate(attrs_report):\n",
    "        print(pfx + (tag.name if i == 0 else ' ' * len(tag.name)), chunk, file = file)\n",
    "    if tag.name in gettext_tags and (text := tag.get_text().strip()):\n",
    "            text_lines = fmt_str(text, 100)\n",
    "            print(pfx + len(tag.name) * ':' + '>-------', file = file)\n",
    "            for text_line in text_lines:\n",
    "                print(pfx + len(tag.name) * ':' + ' ' + text_line, file = file)\n",
    "            print(pfx + len(tag.name) * ':' + '>-------', file = file)\n",
    "    \n",
    "    while stack:\n",
    "        top = stack.pop()\n",
    "    \n",
    "        if top.name in excl_tags:\n",
    "            continue\n",
    "        else:\n",
    "            tree_dump(top, pfx + '  |', gettext_tags = gettext_tags, excl_tags = excl_tags, excl_attrs = excl_attrs, file = file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_dump = False\n",
    "report_dump = True\n",
    "hs_to_report_on_list = ['h' + str(i) for i in range(1, 7)]\n",
    "tags_to_report_on_list = hs_to_report_on_list + ['section', 'article']\n",
    "tags_to_report_on_list_max_wdth = max(len(t) for t in tags_to_report_on_list)\n",
    "\n",
    "with open(FILEDUMP, 'w') as f:\n",
    "    # General Report:\n",
    "    print(\"GENERAL REPORT:\\n\" + '=' * len(\"GENERAL REPORT:\") + '\\n', file = f)\n",
    "    \n",
    "    print(\"\\tQUANTITIES BY TAG:\\n\" + '\\t' + '-' * len(\"QUANTITIES BY TAG:\"), file = f)\n",
    "    print(\"\\t\", end = '', file = f)\n",
    "    for tag_to_report in tags_to_report_on_list:\n",
    "        print(('%-' + str(tags_to_report_on_list_max_wdth) + 's') % tag_to_report + ' = ', len(tuple(soup.find_all(tag_to_report))), sep = '', end = ' | ', file = f)\n",
    "    print(file = f)\n",
    "    print('\\n' + ':' * 160, file = f)\n",
    "    \n",
    "    excl_attrs = ['srcset', 'data-id', 'data-notaid', 'data-source', 'role', 'alt', 'width', 'height', 'loading', 'fetchpriority', 'decoding', 'src']\n",
    "    gettext_tags = ['h' + str(i) for i in range(1, 7)] + ['a'] + ['span'] + ['strong'] + ['time'] + ['p']\n",
    "    \n",
    "    for i, article in enumerate(tags['article']):\n",
    "        print(f'{i:03d}>\\n', file = f)\n",
    "        tree_dump(article, '', gettext_tags = gettext_tags, excl_tags = [], excl_attrs = excl_attrs, file = f)\n",
    "        if html_dump:\n",
    "            print('\\n' + ' ' * 40 + '-' * 40 + '\\n', file = f)\n",
    "            print(article.prettify(), file = f)\n",
    "            print('\\n' + ' ' * 40 + '-' * 40 + '\\n', file = f)\n",
    "        if report_dump:\n",
    "            print('\\n' + ' ' * 40 + '-' * 40 + '\\n', file = f)\n",
    "            # Report goes here:\n",
    "            print('\\t\\t\\t### Report goes here ###', file = f)\n",
    "            print('\\n' + ' ' * 40 + '-' * 40 + '\\n', file = f)\n",
    "        print('\\n' + '=' * 140, file = f)\n",
    "    for i, article in enumerate(tags['article']):\n",
    "        if (h2s_list := list(article.find_all('h2'))):\n",
    "            print(f'<article>[{i}]', *('\\t'+ s for s in (h2.get_text() for h2 in h2s_list)), file = f)\n",
    "    print(':'*80, 'href\\'s\\n', ':' * 160, file = f)\n",
    "    for i, article in enumerate(tags['article']):\n",
    "        if (hrefs_list := list(article.find_all( href = True))):\n",
    "            print(f'<article>[{i}][{len(hrefs_list)}]', *('\\t' + s for s in (href_tag.attrs['href'] for href_tag in hrefs_list)), file = f)    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369c5e762566542b7e1b9087985119ff26b4daf909ad96eeb0bc0767c2e9d06d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
