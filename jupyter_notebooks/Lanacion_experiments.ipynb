{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> || articles : 106 / sections : 274 / headers : 2 / footers : 1 / asides : 5 / h1s : 1 / h2s : 106 / h3s : 30 / h4s : 1 / h5s : 0 / h6s : 0\n"
     ]
    }
   ],
   "source": [
    "import requests, bs4 as bs, lxml, sys\n",
    "sys.path.append('..'); import utils.BSutils\n",
    "\n",
    "httpaddr = \"https://www.lanacion.com.ar/\"\n",
    "FILEDUMP = 'dump/Lanacion_dump.txt'\n",
    "\n",
    "requests_ret = requests.get(httpaddr)\n",
    "\n",
    "print(requests_ret, end = ' || ')\n",
    "soup = bs.BeautifulSoup(requests_ret.text, 'lxml')\n",
    "for discard_tag in (\"script\", \"style\"): # discard <script> and <style> tags\n",
    "    for t in soup.find_all(discard_tag): t.extract()\n",
    "tags_tuple = ('article', 'section', 'header', 'footer', 'aside') + tuple(('h' + str(i) for i in range(1, 7)))\n",
    "tags = {tag_str : soup.find_all(tag_str) for tag_str in tags_tuple}\n",
    "for i, tag in enumerate(tags):\n",
    "    print(f\"{tag}s : {len(tags[tag])}\", sep = '', end = '\\n' if i == len(tags) - 1 else ' / ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These routines outputs a report on a given tag plus all their descendants accompanied with their respective attributes\n",
    "import sys\n",
    "def fmt_attrs(attrs_dict, width = 80, excl_attrs = []):\n",
    "    key_width = max((len(k) for k in attrs_dict)) if attrs_dict else 0\n",
    "    head_width = len(\"{'\") + key_width + len(\"' : '\")\n",
    "    chunk_len = width - head_width\n",
    "    strs_list = []\n",
    "\n",
    "    for i, key in enumerate(attrs_dict):\n",
    "        if key in excl_attrs:\n",
    "            continue\n",
    "        head_str = (\"{'\" if i == 0 else \" '\") + (\"%-\" + str(key_width) + \"s\") % (key, ) + \"' : '\"\n",
    "        val_chunks = tuple((str(attrs_dict[key])[pos : pos + chunk_len] for pos in range(0, len(str(attrs_dict[key])), chunk_len)))\n",
    "        \n",
    "        for (j, chunk) in enumerate(val_chunks):\n",
    "            strs_list.append(\n",
    "                head_str + chunk if j == 0 else\n",
    "                ' ' * head_width + chunk\n",
    "            )\n",
    "        if strs_list:\n",
    "            strs_list.append(strs_list.pop() + \"'\")    \n",
    "\n",
    "    if strs_list: \n",
    "        strs_list.append(strs_list.pop() + \"'\")\n",
    "\n",
    "    return strs_list\n",
    "\n",
    "def fmt_str(text, width):\n",
    "    if hasattr(text, '__str__'):\n",
    "        return (text[i : i + width] for i in range(0, len(text), width))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def tree_dump(tag, pfx, gettext_tags = [], excl_tags = [], excl_attrs = [], file = sys.stdout):\n",
    "    stack = []\n",
    "    stack.extend(reversed([ch for ch in tag.children if isinstance(ch, bs.element.Tag)]))\n",
    "\n",
    "    attrs_report = fmt_attrs(tag.attrs, 135, excl_attrs) if fmt_attrs(tag.attrs, 135, excl_attrs) != [] else ['{}']\n",
    "    for i, chunk in enumerate(attrs_report):\n",
    "        print(pfx + (tag.name if i == 0 else ' ' * len(tag.name)), chunk, file = file)\n",
    "    if tag.name in gettext_tags and (text := tag.get_text().strip()):\n",
    "            text_lines = fmt_str(text, 100)\n",
    "            print(pfx + len(tag.name) * ':' + '>-------', file = file)\n",
    "            for text_line in text_lines:\n",
    "                print(pfx + len(tag.name) * ':' + ' ' + text_line, file = file)\n",
    "            print(pfx + len(tag.name) * ':' + '>-------', file = file)\n",
    "    \n",
    "    while stack:\n",
    "        top = stack.pop()\n",
    "    \n",
    "        if top.name in excl_tags:\n",
    "            continue\n",
    "        else:\n",
    "            tree_dump(top, pfx + '  |', gettext_tags = gettext_tags, excl_tags = excl_tags, excl_attrs = excl_attrs, file = file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 31\n",
      "Cluster[ 0] :  10 ( div) | Cluster[ 1] :   5 (  ul) | Cluster[ 2] :   4 ( div) | Cluster[ 3] :  40 ( div)\n",
      "Cluster[ 4] :   4 ( div) | Cluster[ 5] :   2 ( div) | Cluster[ 6] :   5 ( div) | Cluster[ 7] :   8 ( div)\n",
      "Cluster[ 8] :   3 ( div) | Cluster[ 9] :   5 ( div) | Cluster[10] :   3 ( div) | Cluster[11] :   5 ( div)\n",
      "Cluster[12] :   4 ( div) | Cluster[13] :   6 ( div) | Cluster[14] :   2 ( div) | Cluster[15] :   2 ( div)\n",
      "Cluster[16] :   3 ( div) | Cluster[17] :   7 ( div) | Cluster[18] : 106 (main) | Cluster[19] :   2 ( div)\n",
      "Cluster[20] :   3 ( div) | Cluster[21] :   3 ( div) | Cluster[22] :   3 ( div) | Cluster[23] :   4 ( div)\n",
      "Cluster[24] :   4 ( div) | Cluster[25] :   4 ( div) | Cluster[26] :   4 ( div) | Cluster[27] :   3 ( div)\n",
      "Cluster[28] :   2 ( div) | Cluster[29] :   2 ( div) | Cluster[30] :   4 ( div) | \n",
      " ================================================================================\n",
      "Check for cluster nesting\n",
      "-------------------------\n",
      "Cluster [1] is contained by cluster[0]\n",
      "\n",
      "\n",
      "\n",
      "Cluster [4] is contained by cluster[3]\n",
      "Cluster [5] is contained by cluster[3]\n",
      "Cluster [6] is contained by cluster[3]\n",
      "Cluster [7] is contained by cluster[3]\n",
      "Cluster [8] is contained by cluster[3]\n",
      "Cluster [9] is contained by cluster[3]\n",
      "Cluster [10] is contained by cluster[3]\n",
      "Cluster [11] is contained by cluster[3]\n",
      "Cluster [12] is contained by cluster[3]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cluster [14] is contained by cluster[13]\n",
      "Cluster [15] is contained by cluster[13]\n",
      "Cluster [16] is contained by cluster[13]\n",
      "\n",
      "\n",
      "\n",
      "Cluster [15] is contained by cluster[16]\n",
      "\n",
      "\n",
      "Cluster [0] is contained by cluster[18]\n",
      "Cluster [1] is contained by cluster[18]\n",
      "Cluster [2] is contained by cluster[18]\n",
      "Cluster [3] is contained by cluster[18]\n",
      "Cluster [4] is contained by cluster[18]\n",
      "Cluster [5] is contained by cluster[18]\n",
      "Cluster [6] is contained by cluster[18]\n",
      "Cluster [7] is contained by cluster[18]\n",
      "Cluster [8] is contained by cluster[18]\n",
      "Cluster [9] is contained by cluster[18]\n",
      "Cluster [10] is contained by cluster[18]\n",
      "Cluster [11] is contained by cluster[18]\n",
      "Cluster [12] is contained by cluster[18]\n",
      "Cluster [13] is contained by cluster[18]\n",
      "Cluster [14] is contained by cluster[18]\n",
      "Cluster [15] is contained by cluster[18]\n",
      "Cluster [16] is contained by cluster[18]\n",
      "Cluster [17] is contained by cluster[18]\n",
      "Cluster [19] is contained by cluster[18]\n",
      "Cluster [20] is contained by cluster[18]\n",
      "Cluster [21] is contained by cluster[18]\n",
      "Cluster [22] is contained by cluster[18]\n",
      "Cluster [23] is contained by cluster[18]\n",
      "Cluster [24] is contained by cluster[18]\n",
      "Cluster [25] is contained by cluster[18]\n",
      "Cluster [26] is contained by cluster[18]\n",
      "Cluster [27] is contained by cluster[18]\n",
      "Cluster [28] is contained by cluster[18]\n",
      "Cluster [29] is contained by cluster[18]\n",
      "Cluster [30] is contained by cluster[18]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clusterize articles:\n",
    "clusters_by_article = {}\n",
    "clusters_list = []\n",
    "data_pos_by_article = {}\n",
    "for i, article in enumerate(soup.find_all('article')):\n",
    "    # Tuck away cluster associated with each <article>, accessible by <article>\n",
    "    clusters_by_article[article] = article.parent\n",
    "    while len(clusters_by_article[article].find_all('article')) == 1:\n",
    "        clusters_by_article[article] = clusters_by_article[article].parent  # save cluster, accessible by article\n",
    "    # Tuck away clusters as they appear\n",
    "    if clusters_by_article[article] not in clusters_list:\n",
    "        clusters_list.append(clusters_by_article[article])  # save clusters, as they appear in order\n",
    "    # Compute data-pos for each <article>\n",
    "    data_pos_by_article[article] = article.attrs.get('data-pos')\n",
    "# Report <article>'s per cluster\n",
    "print('Number of clusters:', len(clusters_list))\n",
    "\n",
    "idx_fld_width = len(str(len(clusters_list)))\n",
    "val_fld_width = max(len(str(len(cluster.find_all('article')))) for cluster in clusters_list)\n",
    "art_fld_width = len(tuple(soup.find_all('article')))\n",
    "\n",
    "for i, cluster in enumerate(clusters_list):\n",
    "    print(('Cluster[%' + str(idx_fld_width) + 'd] : %' + str(val_fld_width) + 'd') % (i, len(cluster.find_all('article'))), \\\n",
    "        '(%4s' % cluster.name,\n",
    "        end = ')\\n' if (i + 1) % 4 == 0 else ') | ')\n",
    "print('\\n', '=' * 80)\n",
    "\n",
    "# Check for cluster nesting:\n",
    "print(\"Check for cluster nesting\\n\", len(\"Check for cluster nesting\") * '-', sep = '')\n",
    "for i, cluster_i in enumerate(clusters_list):\n",
    "    for j, cluster_j in enumerate(clusters_list):\n",
    "        if cluster_j is cluster_i:\n",
    "            continue\n",
    "        if cluster_j in cluster_i.find_all(True):\n",
    "            print(f'Cluster [{j}] is contained by cluster[{i}]')\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dump/Lanacion_dump.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m tags_to_report_on_list_max_wdth \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mlen\u001b[39m(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tags_to_report_on_list)\n\u001b[0;32m      7\u001b[0m final_report \u001b[39m=\u001b[39m {}\n\u001b[1;32m----> 9\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(FILEDUMP, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     10\u001b[0m     \u001b[39m# General Report:\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGENERAL REPORT:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGENERAL REPORT:\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, file \u001b[39m=\u001b[39m f)\n\u001b[0;32m     13\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mQUANTITIES BY TAG:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mQUANTITIES BY TAG:\u001b[39m\u001b[39m\"\u001b[39m), file \u001b[39m=\u001b[39m f)\n",
      "File \u001b[1;32mc:\\code\\rm\\BSscrapper\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dump/Lanacion_dump.txt'"
     ]
    }
   ],
   "source": [
    "html_dump = False; report_dump = True; exploration_dump = False\n",
    "\n",
    "hs_to_report_on_list = ['h' + str(i) for i in range(1, 7)]\n",
    "tags_to_report_on_list = hs_to_report_on_list + ['section', 'article']\n",
    "tags_to_report_on_list_max_wdth = max(len(t) for t in tags_to_report_on_list)\n",
    "\n",
    "final_report = {}\n",
    "\n",
    "with open(FILEDUMP, 'w') as f:\n",
    "    # General Report:\n",
    "    print(\"GENERAL REPORT:\\n\" + '=' * len(\"GENERAL REPORT:\") + '\\n', file = f)\n",
    "    \n",
    "    print(\"\\tQUANTITIES BY TAG:\\n\" + '\\t' + '-' * len(\"QUANTITIES BY TAG:\"), file = f)\n",
    "    print(\"\\t\", end = '', file = f)\n",
    "    for tag_to_report in tags_to_report_on_list:\n",
    "        print(('%-' + str(tags_to_report_on_list_max_wdth) + 's') % tag_to_report + ' = ', len(tuple(soup.find_all(tag_to_report))), sep = '', end = ' | ', file = f)\n",
    "    print(file = f)\n",
    "    print('\\n' + ':' * 160, file = f)\n",
    "    \n",
    "    excl_attrs = ['srcset', 'data-id', 'data-notaid', 'data-source', 'role', 'alt', 'width', 'height', 'loading', 'fetchpriority', 'decoding', 'src']\n",
    "    gettext_tags = ['h' + str(i) for i in range(1, 7)] + ['a'] + ['span'] + ['strong'] + ['time'] + ['section'] + ['p']\n",
    "    \n",
    "    article_reports = {}\n",
    "    for i, article in enumerate(tags['article']):\n",
    "        article_report = {}\n",
    "        article_report['UKEY'] = '### PLACEHOLDER'\n",
    "        article_report['JOB'] = 'LanacionJob'\n",
    "        \n",
    "        article_report['ARTICLE'] = i\n",
    "        article_report['TITLE'] = article.find(class_ = 'com-title').get_text()\n",
    "        article_report['TITLE_WORD_COUNT'] = len(tuple(article_report['TITLE'].split(' ')))\n",
    "\n",
    "        article_report['LANACION_data_pos'] = data_pos_by_article[article]\n",
    "        article_report['LANACION_data_pos_cluster'] = article_report['LANACION_data_pos'][:-2]\n",
    "        article_report['LANACION_data_pos_cluster_member'] = article_report['LANACION_data_pos'][-2:]\n",
    "        article_report['LANACION_hrefs_list'] = list(hrefs_set := set(href_tag.get('href') for href_tag in article.find_all(href = True)))\n",
    "        \n",
    "        # article_report['CLUSTER'] = clusters_list.index(clusters_by_article[article])\n",
    "        # article_report['CLUSTER_INDEX'] = tuple(clusters_by_article[article].find_all('article')).index(article)\n",
    "        # article_report['CLUSTER_SIZE'] = len(tuple(clusters_by_article[article].find_all('article')))\n",
    "        # article_report['CLUSTER_UNIQUE'] = tuple((article in cluster.find_all(True)) for cluster in clusters_list).count(True)\n",
    "        \n",
    "        author_text = author_tags[0].get_text() if (author_tags := list(article.find_all('strong'))) else None\n",
    "        article_report['AUTHOR'] = author_text\n",
    "        article_report['SUMMARY'] = t[0].get_text() if (t := tuple(article.find_all(class_ = 'com-subhead'))) else None\n",
    "        article_report['VOLANTA'] = lead_tag.get_text() if (lead_tag := article.find(class_ = 'com-lead')) else None\n",
    "        \n",
    "\n",
    "        if hrefs_set:\n",
    "            internal_hrefs_set = {href for href in hrefs_set if href[0] == '/'}\n",
    "            external_hrefs_set = hrefs_set - internal_hrefs_set\n",
    "            \n",
    "            if internal_hrefs_set:\n",
    "                article_report['SLUG'] = (slug := list(internal_hrefs_set)[0])\n",
    "                article_report['SLUG_INTERNAL'] = True\n",
    "                \n",
    "                category_list = slug.split('/')\n",
    "                if len(category_list) >= 3:\n",
    "                    article_report['CATEGORY'] = category_list[1]\n",
    "                    if len(category_list) > 3:\n",
    "                        article_report['SUBCATEGORY'] = category_list[2]\n",
    "                    else:\n",
    "                        article_report['SUBCATEGORY'] = None\n",
    "                else:\n",
    "                    article_report['CATEGORY'] = None\n",
    "                    article_report['SUBCATEGORY'] = None\n",
    "                \n",
    "            elif external_hrefs_set:\n",
    "                article_report['SLUG'] = list(external_hrefs_set)[0]\n",
    "                article_report['SLUG_INTERNAL'] = False\n",
    "        \n",
    "        else:\n",
    "            article_report['SLUG'] = None\n",
    "            article_report['SLUG_INTERNAL'] = None\n",
    "            article_report['CATEGORY'] = None\n",
    "            article_report['SUBCATEGORY'] = None\n",
    "        \n",
    "        article_report['Origen'] = None\n",
    "        article_report['FechaFiltro'] = None\n",
    "        article_report['FechaCreacion'] = None\n",
    "        article_report['FechaModificacion'] = None\n",
    "\n",
    "        article_reports[article] = article_report\n",
    "    \n",
    "    for i, article in enumerate(tags['article']):\n",
    "        print('\\n' + '=' * 160, file = f)\n",
    "        print(f'{i:03d}>\\n', file = f)\n",
    "        tree_dump(article, '', gettext_tags = gettext_tags, excl_tags = [], excl_attrs = excl_attrs, file = f)\n",
    "        if html_dump:\n",
    "            print('\\n' + ' ' * 40 + '-' * 40 + '\\n', file = f)\n",
    "            print(article.prettify(), file = f)\n",
    "            print('\\n' + ' ' * 40 + '-' * 40 + '\\n', file = f)\n",
    "        if report_dump:\n",
    "            print('\\n' + ' ' * 40 + '-' * 40 + '\\n', file = f)\n",
    "            # Report goes here:\n",
    "            key_field_width = max(len(k) for k, v in article_reports[article].items() if v)\n",
    "            for key, val in article_reports[article].items():\n",
    "                if val != None:\n",
    "                    print((('\\t%' + str(key_field_width) + 's') % key) + ' :', val, file = f)\n",
    "            # tree_dump(clusters_dict[article], '', gettext_tags = ['h' + str(i) for i in range(1, 7)], excl_tags = ['a'], excl_attrs = excl_attrs, file = f)\n",
    "            if exploration_dump:\n",
    "                article_titles = tuple(article.find_all(class_ = 'com-title'))\n",
    "                article_links = tuple(article.find_all(class_ = 'com-link'))\n",
    "                article_subhead = tuple(article.find_all(class_ = 'com-subhead'))\n",
    "                article_lead = tuple(article.find_all(class_ = 'com-lead'))\n",
    "                article_hour = tuple(article.find_all(class_ = 'com-hour'))\n",
    "                if len(article_titles):\n",
    "                    print('# com-title :', len(article_titles), file = f)\n",
    "                    print('com-title :', (article_titles), file = f)\n",
    "                if len(article_links):\n",
    "                    print('# com-link :', len(article_links), file = f)\n",
    "                    print('com-link :', (article_links), file = f)\n",
    "                if len(article_subhead):\n",
    "                    print('# com-subhead :', len(article_subhead), file = f)\n",
    "                    print('com-subhead :', (article_subhead), file = f)\n",
    "                if len(article_lead):\n",
    "                    print('# com-lead :', len(article_lead), file = f)\n",
    "                    print('com-lead :', (article_lead), file = f)\n",
    "                if len(article_hour):\n",
    "                    print('# com-hour :', len(article_hour), file = f)\n",
    "                    print('com-hour :', (article_hour), file = f)\n",
    "            print('\\n' + ' ' * 40 + '-' * 40 + '\\n', file = f)\n",
    "        \n",
    "    print(('\\n' + ':' * 160), '\\n:::: FINAL REPORT ::::', '\\n' + ':' * 160, sep = '', file = f)\n",
    "\n",
    "    print(':::: SLUGS ::::', file = f)\n",
    "    for i, j in ((val['ARTICLE'], val['LANACION_hrefs_list']) for (key, val) in article_reports.items()):\n",
    "        print(f'article [{str(i):3s}] ({len(j)}):: {j} ', file = f)\n",
    "    \n",
    "    print(':::: AUTHORS ::::', file = f)\n",
    "    for i, j in ((val['ARTICLE'], val['AUTHOR']) for (key, val) in article_reports.items()):\n",
    "        print(f'article [{str(i):3s}] : AUTHOR :: {j} ', file = f)\n",
    "\n",
    "    print(':::: AUTHORS INSPECTION ::::', file = f)\n",
    "    c = 0\n",
    "    for i, article in enumerate(tags['article']):\n",
    "        author_presumptive_tags_list = [tag.get_text() for tag in article.find_all('strong')]\n",
    "        if author_presumptive_tags_list:\n",
    "            c += 1\n",
    "            print(f'<article>[{i}] :', author_presumptive_tags_list[0], f':: {c}', file = f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369c5e762566542b7e1b9087985119ff26b4daf909ad96eeb0bc0767c2e9d06d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
